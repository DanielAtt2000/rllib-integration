2022-12-11 15:27:23,944	WARNING worker.py:1839 --
 A worker died or was killed while executing a task by an unexpected system error.
  To troubleshoot the problem, check the logs for the dead worker.
  RayTask ID: ffffffffffffffffc5fa41da08d10910970740c001000000
  Worker ID: b0993b99bf78ac5da5170c279b55f097e62bf09b71aba0a7caae8797
  Node ID: 1c176b402b0851f2fe6dba839c8ddd5f7e6ac3d017bd9404c3dcb117
  Worker IP address: 192.168.1.205
   Worker port: 36777
    Worker PID: 3606
    Worker exit type: SYSTEM_ERROR W
    orker exit detail: Worker unexpectedly exits with a connection error code 2.
    End of file.
    There are some potential root causes.
     (1) The process is killed by SIGKILL by OOM killer due to high memory usage.zz`
     (2) ray stop --force is called.
     (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(RolloutWorker pid=3606) Reward: 0
(CustomDQNTrainer pid=3561) 2022-12-11 15:27:58,796	WARNING algorithm.py:2610 -- Worker crashed during training or evaluation! To try to continue without failed worker(s), set `ignore_worker_failures=True`. To try to recover the failed worker(s), set `recreate_failed_workers=True`.
== Status ==
Current time: 2022-12-11 15:28:02 (running for 05:23:07.59)
Memory usage on this node: 6.3/15.3 GiB
Using FIFO scheduling algorithm.
Resources requested: 12.0/12 CPUs, 1.0/1 GPUs, 0.0/7.8 GiB heap, 0.0/3.9 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/daniel/ray_results/carla_rllib/dqn_f406becfb4
Number of trials: 1/1 (1 RUNNING)
+---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------+
| Trial name                            | status   | loc                |   iter |   total time (s) |    ts |    reward |   num_recreated_worker |   episode_reward_max |   episode_reward_min |
|                                       |          |                    |        |                  |       |           |                      s |                      |                      |
|---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------|
| CustomDQNTrainer_CarlaEnv_e0d5c_00000 | RUNNING  | 192.168.1.205:3561 |     65 |          18973.3 | 65520 | -0.194836 |                      0 |             0.304382 |            -0.796992 |
+---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------+


2022-12-11 15:28:03,460	ERROR trial_runner.py:993 -- Trial CustomDQNTrainer_CarlaEnv_e0d5c_00000: Error processing event.
Traceback (most recent call last):
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/tune/execution/ray_trial_executor.py", line 1050, in get_next_executor_event
    future_result = ray.get(ready_future)
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
    return func(*args, **kwargs)
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/_private/worker.py", line 2289, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError: ray::CustomDQNTrainer.train() (pid=3561, ip=192.168.1.205, repr=CustomDQNTrainer)
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 355, in train
    raise skipped from exception_cause(skipped)
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/tune/trainable/trainable.py", line 352, in train
    result = self.step()
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py", line 772, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py", line 2953, in _run_one_training_iteration
    num_recreated += self.try_recover_from_step_attempt(
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py", line 2617, in try_recover_from_step_attempt
    raise error
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/algorithms/algorithm.py", line 2948, in _run_one_training_iteration
    results = self.training_step()
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/algorithms/dqn/dqn.py", line 379, in training_step
    new_sample_batch = synchronous_parallel_sample(
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/execution/rollout_ops.py", line 100, in synchronous_parallel_sample
    sample_batches = ray.get(
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: RolloutWorker
	actor_id: c5fa41da08d10910970740c001000000
	pid: 3606
	namespace: 4b34fd9e-18ce-493b-a2fe-db47d98addcb
	ip: 192.168.1.205
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
Result for CustomDQNTrainer_CarlaEnv_e0d5c_00000:
  agent_timesteps_total: 65520
  counters:
    last_target_update_ts: 65520
    num_agent_steps_sampled: 65520
    num_agent_steps_trained: 64528
    num_env_steps_sampled: 65520
    num_env_steps_trained: 64528
    num_target_updates: 127
  custom_metrics:
    angle_with_center_max: 0.3896300494670868
    angle_with_center_mean: 0.19162239134311676
    angle_with_center_min: 0.027115046977996826
  date: 2022-12-11_15-22-05
  done: false
  episode_len_mean: 611.0
  episode_media: {}
  episode_reward_max: 0.3043824637780539
  episode_reward_mean: -0.1948358957501928
  episode_reward_min: -0.7969918949018165
  episodes_this_iter: 2
  episodes_total: 108
  experiment_id: c0e6e887753b4d58acb2bd9bbfb9d369
  experiment_tag: '0'
  hostname: daniel-AORUS-5-SB
  info:
    last_target_update_ts: 65520
    learner:
      default_policy:
        custom_metrics: {}
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.1
          grad_gnorm: 40.0
          max_q: 303461952.0
          mean_q: 303461920.0
          min_q: 303461952.0
        mean_td_error: 13457344.0
        model: {}
        num_agent_steps_trained: 16.0
        td_error: [13457344.0, 13457344.0, 13457344.0, 13457344.0, 13457344.0, 13457344.0,
          13457344.0, 13457344.0, 13457344.0, 13457344.0, 13457344.0, 13457344.0, 13457344.0,
          13457344.0, 13457344.0, 13457344.0]
    num_agent_steps_sampled: 65520
    num_agent_steps_trained: 64528
    num_env_steps_sampled: 65520
    num_env_steps_trained: 64528
    num_target_updates: 127
  iterations_since_restore: 65
  node_ip: 192.168.1.205
  num_agent_steps_sampled: 65520
  num_agent_steps_trained: 64528
  num_env_steps_sampled: 65520
  num_env_steps_sampled_this_iter: 1008
  num_env_steps_trained: 64528
  num_env_steps_trained_this_iter: 1008
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 1008
  perf:
    cpu_util_percent: 3.3825665859564165
    ram_util_percent: 61.98523002421308
  pid: 3561
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.04664072703710361
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 250.36574337441292
    mean_inference_ms: 3.095084288232517
    mean_raw_obs_processing_ms: 4.606016170045591
  sampler_results:
    custom_metrics:
      angle_with_center_max: 0.3896300494670868
      angle_with_center_mean: 0.19162239134311676
      angle_with_center_min: 0.027115046977996826
    episode_len_mean: 611.0
    episode_media: {}
    episode_reward_max: 0.3043824637780539
    episode_reward_mean: -0.1948358957501928
    episode_reward_min: -0.7969918949018165
    episodes_this_iter: 2
    hist_stats:
      episode_lengths: [611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611, 611,
        611, 611, 611, 611, 611, 611, 611, 611]
      episode_reward: [-0.7969918949018165, -0.7969918949018165, -0.7969918949018165,
        -0.5524231163092197, 0.3043824637780539, -0.5524231163092197, 0.3043824637780539,
        0.3043824637780539, 0.3043824637780539, 0.3043824637780539, 0.3043824637780539,
        -0.5524231163092197, 0.3043824637780539, 0.3043824637780539, -0.5524231163092197,
        0.218707461376215, -0.5524231163092197, -0.5524231163092197, 0.3043824637780539,
        -0.5524231163092197, 0.218707461376215, -0.5524231163092197, -0.7969918949018165,
        0.3043824637780539, 0.3043824637780539, 0.3043824637780539, -0.5524231163092197,
        -0.7969918949018165, -0.5524231163092197, -0.5524231163092197, 0.3043824637780539,
        -0.5524231163092197, 0.3043824637780539, 0.218707461376215, 0.218707461376215,
        -0.5524231163092197, 0.3043824637780539, 0.218707461376215, -0.5524231163092197,
        0.3043824637780539, -0.5524231163092197, 0.3043824637780539, 0.218707461376215,
        -0.5524231163092197, 0.3043824637780539, 0.3043824637780539, 0.218707461376215,
        0.3043824637780539, -0.5524231163092197, -0.5524231163092197, 0.218707461376215,
        -0.7969918949018165, 0.218707461376215, 0.218707461376215, -0.7969918949018165,
        -0.7969918949018165, 0.3043824637780539, 0.3043824637780539, -0.7969918949018165,
        -0.7969918949018165, 0.3043824637780539, 0.218707461376215, 0.3043824637780539,
        0.3043824637780539, 0.3043824637780539, 0.3043824637780539, -0.7969918949018165,
        -0.5524231163092197, 0.3043824637780539, -0.5524231163092197, -0.5524231163092197,
        -0.5524231163092197, -0.7969918949018165, -0.7969918949018165, -0.7969918949018165,
        -0.7969918949018165, 0.218707461376215, -0.7969918949018165, -0.7969918949018165,
        0.3043824637780539, 0.218707461376215, 0.3043824637780539, -0.7969918949018165,
        -0.5524231163092197, -0.7969918949018165, -0.5524231163092197, -0.7969918949018165,
        -0.7969918949018165, -0.7969918949018165, 0.3043824637780539, -0.5524231163092197,
        -0.5524231163092197, 0.3043824637780539, 0.218707461376215, 0.218707461376215,
        -0.7969918949018165, 0.3043824637780539, -0.5524231163092197, 0.218707461376215,
        0.218707461376215]
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf:
      mean_action_processing_ms: 0.04664072703710361
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 250.36574337441292
      mean_inference_ms: 3.095084288232517
      mean_raw_obs_processing_ms: 4.606016170045591
  time_since_restore: 18973.331011533737
  time_this_iter_s: 290.27971863746643
  time_total_s: 18973.331011533737
  timers:
    learn_throughput: 36.773
    learn_time_ms: 435.107
    load_throughput: 118860.9
    load_time_ms: 0.135
    synch_weights_time_ms: 1.72
    training_iteration_time_ms: 4607.386
  timestamp: 1670768525
  timesteps_since_restore: 0
  timesteps_total: 65520
  training_iteration: 65
  trial_id: e0d5c_00000
  warmup_time: 15.063173532485962

(CustomDQNTrainer pid=3561) 2022-12-11 15:28:04,905	ERROR worker_set.py:591 -- Failed to stop workers!
(CustomDQNTrainer pid=3561) Traceback (most recent call last):
(CustomDQNTrainer pid=3561)   File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py", line 589, in stop
(CustomDQNTrainer pid=3561)     ray.get(tids)
(CustomDQNTrainer pid=3561)   File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/_private/client_mode_hook.py", line 105, in wrapper
(CustomDQNTrainer pid=3561)     return func(*args, **kwargs)
(CustomDQNTrainer pid=3561)   File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/_private/worker.py", line 2291, in get
(CustomDQNTrainer pid=3561)     raise value
(CustomDQNTrainer pid=3561) ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
(CustomDQNTrainer pid=3561) 	class_name: RolloutWorker
(CustomDQNTrainer pid=3561) 	actor_id: c5fa41da08d10910970740c001000000
(CustomDQNTrainer pid=3561) 	pid: 3606
(CustomDQNTrainer pid=3561) 	namespace: 4b34fd9e-18ce-493b-a2fe-db47d98addcb
(CustomDQNTrainer pid=3561) 	ip: 192.168.1.205
(CustomDQNTrainer pid=3561) The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
== Status ==
Current time: 2022-12-11 15:29:22 (running for 05:24:27.50)
Memory usage on this node: 5.5/15.3 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/12 CPUs, 0/1 GPUs, 0.0/7.8 GiB heap, 0.0/3.9 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /home/daniel/ray_results/carla_rllib/dqn_f406becfb4
Number of trials: 1/1 (1 ERROR)
+---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------+
| Trial name                            | status   | loc                |   iter |   total time (s) |    ts |    reward |   num_recreated_worker |   episode_reward_max |   episode_reward_min |
|                                       |          |                    |        |                  |       |           |                      s |                      |                      |
|---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------|
| CustomDQNTrainer_CarlaEnv_e0d5c_00000 | ERROR    | 192.168.1.205:3561 |     65 |          18973.3 | 65520 | -0.194836 |                      0 |             0.304382 |            -0.796992 |
+---------------------------------------+----------+--------------------+--------+------------------+-------+-----------+------------------------+----------------------+----------------------+
Number of errored trials: 1
+---------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------+
| Trial name                            |   # failures | error file                                                                                                                |
|---------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------|
| CustomDQNTrainer_CarlaEnv_e0d5c_00000 |            1 | /home/daniel/ray_results/carla_rllib/dqn_f406becfb4/CustomDQNTrainer_CarlaEnv_e0d5c_00000_0_2022-12-11_10-04-54/error.txt |
+---------------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------+


done.
Traceback (most recent call last):
  File "dqn_train.py", line 119, in <module>
    main()
  File "dqn_train.py", line 114, in main
    run(args)
  File "dqn_train.py", line 39, in run
    tune.run(CustomDQNTrainer,
  File "/home/daniel/anaconda3/envs/CarlaRlib/lib/python3.8/site-packages/ray/tune/tune.py", line 771, in run
    raise TuneError("Trials did not complete", incomplete_trials)
ray.tune.error.TuneError: ('Trials did not complete', [CustomDQNTrainer_CarlaEnv_e0d5c_00000])
(CarlaRlib) daniel@daniel-AORUS-5-SB:~/rllib-integration$